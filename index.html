<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="description"
    content="Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation">
  <meta name="keywords" content="Zero-shot Generalization, Visuomotor Policies, Data Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Training-Free Generation of Temporally Consistent Rewards from VLMs</title>


  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <!-- <link href="./static/css/googlecss.css"
        rel="stylesheet"> -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  
    <!-- 引入 MathJax 库 -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
  <script src="./static/js/jquerymin351.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .author-block {
        display: block;
    }

</style>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Training-Free Generation of Temporally Consistent Rewards from VLMs </h1>
            <div class="is-size-5 publication-authors">
                <!-- <span class="team-name"><b>Beijing Institute of Technology <p style="font-size: 70%"></p></b></span> -->
                <span class="author-block">Anonymous
                <div class="is-size-6 publication-authors"> 
                  <span class="author-block">
  
                  </span>         
                </div>
              </div>
              
        
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
         <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
       

      
      <!-- </div>
    </div>
  </section>  -->

  <section class="section" id="single-task-1">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in vision-language models (VLMs) have
              significantly improved performance in embodied tasks such
              as goal decomposition and visual comprehension. However,
              providing accurate rewards for robotic manipulation without fine-tuning VLMs remains challenging due to the absence of domain-specific robotic knowledge in pre-trained
              datasets and high computational costs that hinder realtime applicability. To address this, 
              we propose $ T^2-VLM $,a novel training-free, temporally consistent framework that
              generates accurate rewards through tracking the changes in VLM-derived subgoals. Specifically, our method first
              queries the VLM to establish spatially aware subgoals and
              an initial completion estimate before each rounds of interaction. We then employ a Bayesian tracking algorithm to
              update the completion status dynamically, using spatial hidden states to generate structured rewards for reinforcement
              learning (RL) agents. This approach enhances long-horizon decision-making and improves failure recovery capabilities with RL. Extensive experiments indicate that $ T^2-VLM $
              achieves state-of-the-art performance in two robot manipulation benchmarks, 
              demonstrating superior reward accuracy with reduced computation consumption. We believe
              our approach not only advances reward generation techniques but also contributes to the broader field of embodied
              AI.
          </div>
        </div>
      </div>
      <br>
      <!--/ Abstract. -->

      <!-- Hardware Setup. -->    
      <!-- / Hardware Setup. -->


      <!-- RoboMIND Data Analysis. -->
  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
            <h2 class="title is-3">Method</h2>
            <!-- <div class="container is-max-desktop"> -->
            <div class="hero-body">
              <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/teaser.mp4" type="video/mp4">
              </video> -->
              <div class="yush-div-center">
                <img src="./static/images/overview.jpg" class="img-responsive" style="width: 100%; height: auto;">
              </div>
              <div class="content has-text-justified">
                <p>
                  In this work, we introduce T2-VLM, a novel reward generation framework that ensures temporal consistency by tracking the environment's goal completion status.
                  Fig. 2 provides an overview of T2-VLM. Unlike previous VLM-based reward generation methods, T2-VLM queries the VLM only once per episode,
                  then incorporates temporal information (object trajectories) to update the VLM-initialized goal status. A bayesian tracking algorithm is then
                  proposed to manage the updates, enabling T2-VLM to remain training-free, deliver precise reinforcement learning
                  (RL) rewards, and withstand inaccurate VLM estimations by leveraging temporal data. In this section, we begin by
                  explaining how VLMs are prompted to generate subgoals and initial estimations, then describe a particle filter method
                  that uses VLM-coded affordances for posterior computation, and finally present T2-VLM's implementation details.
              </div>
            </div>
          <!-- </div> -->
        </div>
      </div>
    </div>
    <!-- / RoboMIND Data Analysis. -->
  </section>

  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <!-- Experiment  -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
      <!-- Experiment  -->
            <p> 
              In this section, we aim to answer the following research
              questions. 1. Is the reward generated by T2-VLM sufficient to support RL training? 2. How do the policies trained by 
              T2-VLM compare with other methods in recovery capabilities? 3. How does each component contribute to T2-VLM?
              To solve these questions, we conducted experiments on two robot benchmarks with four manipulation tasks.
            </p>
          <h2 class="title is-4">Setup</h2>
          <p> 
            We conducted experiments on both offline datasets and online environments to comprehensively explore the reward 278
            accuracy and recovery ability provided by T2
            -VLM. We introduce four tasks from two commonly used benchmark 
            Cliport [24] and Calvin [18]: Place-same-color, Stack
            tower, Make-line and Cleanup-desk. They are all long- 
            horizon tasks that involves multiple subgoals. The success 
            conditions for all four tasks are: 284
            1. Place-same-color: The red cube is inside the red bowl; 
            the green cube is inside the green bowl; The yellow cube 
            is on the tray; the blue cube is on the tray. 
            2. Stack-tower: The red cube is on the desktop; the yellow 
            cube is on the red cube; the gray cube is on the yellow 
            cube; the green cube is on the gray cube. 
            3. Make-lines: The red cube is next to the left of the green 
            cube; The green cube is next to the left of the blue cube. 
            4. Cleanup-desk: The drawer is open; the red cube is on the 
            drawer; the blue cube is on the drawer; 
            More detail descriptions on the tasks can be found in the  
            appendix. 
            To evaluate the accuracy for identify subgoal completion 
            status, we constructed four offline datasets, each consists 
            of 50 trajectories, collected by a mixture of scripted expert 
            policy and random policy. 
            During online reinforcement learning training, we com- 
            pare T2-VLM with other two state-of-that-art (SOTA) re- 
            ward generation methods, as well as the environmental re- 
            wards. They two comparison methods are: 1) VLM-CaR: 
            generates reward by querying VLMs to write codes based 
            on the images and language task description before the 
            robot execution. Then, it utilize a small number of trajecto- 
            ries that generated from expert and random policies to cor
          </p>
        <br>
        <div class="yush-div-center">
          <img src="./static/images/dmcgb_table.jpg" class="img-responsive">
        </div>
        <br>
        <h2 class="title is-4">Real-world Experiments</h2>
        <p>To validate $ T^2-VLM $ ...
        </p>

        <div class="yush-div-center">
          <img src="./static/images/table2.jpg" class="img-responsive">
        </div>
      <br>

      <div id="single-task">
        <!-- Performance on Single Tasks.. -->
         <br>
        <!-- <h3 class="title is-4">Success Examples of ACT on Single Tasks</h3>  -->
        <!-- <p> <b>Single Task Results</b>. ACT achieves an average success rate of 30.7% (Franka), 34.0% (Tien Kung), 55.3% (AgileX) and 38.0% (UR-5e).
        </p> -->
        <!-- <video width="640" height="360" controls>
          <source src="your-video.mp4" type="video/mp4">
      </video> -->

        <div class="content has-text-justified">
            <div class="columns">
              <div class="column has-text-centered">
                <div class="yush-div-center">
                  <img src="./static/images/arm1.jpg" class="img-responsive" height="75%">
                </div>
                <p style="font-size: 125%"><b>Training</b></p>

            </div>
              <div class="column has-text-centered">
                  
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/video2.mp4" type="video/mp4">
                  </video>
                  <p style="font-size: 125%"><b>Testing<br>(Change Background)</b></p>
              </div>
              <div class="column has-text-centered">
                  
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/video3.mp4" type="video/mp4">
                  </video>
                  <p style="font-size: 125%"><b>Testing <br>(Add Distractors)</b></p>
              </div>
             

          </div>
         
        </div>

      </div>
      <br>
  </section>

  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <!-- Experiment  -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Conclusion</h2>
      <!-- Experiment  -->
      <p> In this paper ...
      </p>
      <br>

      <div id="single-task">
        <!-- Performance on Single Tasks.. -->
         <br>
        <!-- <h3 class="title is-4">Success Examples of ACT on Single Tasks</h3>  -->
        <!-- <p> <b>Single Task Results</b>. ACT achieves an average success rate of 30.7% (Franka), 34.0% (Tien Kung), 55.3% (AgileX) and 38.0% (UR-5e).
        </p> -->
        <!-- <video width="640" height="360" controls>
          <source src="your-video.mp4" type="video/mp4">
      </video> -->
      </div>
      <br>



  </section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
              and <a href="https://eureka-research.github.io/">Eureka</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
