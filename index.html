<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="description"
    content="Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation">
  <meta name="keywords" content="Zero-shot Generalization, Visuomotor Policies, Data Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Training-Free Generation of Temporally Consistent Rewards from VLMs</title>


  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <!-- <link href="./static/css/googlecss.css"
        rel="stylesheet"> -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="./static/js/jquerymin351.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .author-block {
        display: block;
    }

</style>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Training-Free Generation of Temporally Consistent Rewards from VLMs </h1>
            <div class="is-size-5 publication-authors">
                <!-- <span class="team-name"><b>Beijing Institute of Technology <p style="font-size: 70%"></p></b></span> -->
                <span class="author-block">Yinuo Zhao<sup>1,2</sup>, Kun Wu<sup>2,2</sup>,Tianjiao Yi<sup>1</sup>,
                Zhiyuan Xu<sup>2</sup>, Zhengping Che<sup>2</sup>, <span class="author-block"></span>Chi Harold Liu<sup>1</sup>,Jian Tang<sup>2</sup>
    
                   

                <div class="is-size-6 publication-authors"> 
                  <span class="author-block">
  
                  </span>
                 <br>
               
                <div class="is-size-5 publication-authors id=institute">
                  <sup>1</sup>Beijing Institute of Technology, 
                  <sup>2</sup>Beijing Innovation Center of Humanoid Robotics
  
                </div>
                 <br>           
                </div>
              </div>
              
        
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
         <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
       

      
      <!-- </div>
    </div>
  </section>  -->

  <section class="section" id="single-task-1">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in vision-language models (VLMs) have
              significantly improved performance in embodied tasks such
              as goal decomposition and visual comprehension. However,
              providing accurate rewards for robotic manipulation without fine-tuning VLMs remains challenging due to the absence of domain-specific robotic knowledge in pre-trained
              datasets and high computational costs that hinder realtime applicability. To address this, 
              we propose T2-VLM,a novel training-free, temporally consistent framework that
              generates accurate rewards through tracking the changes in VLM-derived subgoals. Specifically, our method first
              queries the VLM to establish spatially aware subgoals and
              an initial completion estimate before each rounds of interaction. We then employ a Bayesian tracking algorithm to
              update the completion status dynamically, using spatial hidden states to generate structured rewards for reinforcement
              learning (RL) agents. This approach enhances long-horizon decision-making and improves failure recovery capabili
              ties with RL. Extensive experiments indicate that T2-VLM
              achieves state-of-the-art performance in two robot manipulation benchmarks, 
              demonstrating superior reward accuracy with reduced computation consumption. We believe
              our approach not only advances reward generation techniques but also contributes to the broader field of embodied
              AI.
          </div>
        </div>
      </div>
      <br>
      <!--/ Abstract. -->

      <!-- Hardware Setup. -->
    
      <!-- / Hardware Setup. -->


      <!-- RoboMIND Data Analysis. -->
  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
            <h2 class="title is-3">Method</h2>
            <!-- <div class="container is-max-desktop"> -->
            <div class="hero-body">
              <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/teaser.mp4" type="video/mp4">
              </video> -->
              <div class="yush-div-center">
                <img src="./static/images/overview.jpg" class="img-responsive" style="width: 100%; height: auto;">
              </div>
              <div class="content has-text-justified">
                <p>
                  We introduce 
              </div>
            </div>
          <!-- </div> -->
        </div>
      </div>
    </div>
    <!-- / RoboMIND Data Analysis. -->
  </section>

  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <!-- Experiment  -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
      <!-- Experiment  -->
        <h2 class="title is-4">Experiments on DMC-GB</h2>
        <p> 
          As shown in Tab. 1, T2-VLM achieves ...
        </p>
        <br>
        <div class="yush-div-center">
          <img src="./static/images/dmcgb_table.jpg" class="img-responsive">
        </div>
        <br>
        <h2 class="title is-4">Real-world Experiments</h2>
        <p>To validate T2-VLM ....
        </p>

        <div class="yush-div-center">
          <img src="./static/images/table2.jpg" class="img-responsive">
        </div>
      <br>

      <div id="single-task">
        <!-- Performance on Single Tasks.. -->
         <br>
        <!-- <h3 class="title is-4">Success Examples of ACT on Single Tasks</h3>  -->
        <!-- <p> <b>Single Task Results</b>. ACT achieves an average success rate of 30.7% (Franka), 34.0% (Tien Kung), 55.3% (AgileX) and 38.0% (UR-5e).
        </p> -->
        <!-- <video width="640" height="360" controls>
          <source src="your-video.mp4" type="video/mp4">
      </video> -->

        <div class="content has-text-justified">
            <div class="columns">
              <div class="column has-text-centered">
                <div class="yush-div-center">
                  <img src="./static/images/arm1.jpg" class="img-responsive" height="75%">
                </div>
                <p style="font-size: 125%"><b>Training</b></p>

            </div>
              <div class="column has-text-centered">
                  
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/video2.mp4" type="video/mp4">
                  </video>
                  <p style="font-size: 125%"><b>Testing<br>(Change Background)</b></p>
              </div>
              <div class="column has-text-centered">
                  
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/video3.mp4" type="video/mp4">
                  </video>
                  <p style="font-size: 125%"><b>Testing <br>(Add Distractors)</b></p>
              </div>
             

          </div>
         
        </div>

      </div>
      <br>
  </section>

  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <!-- Experiment  -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Conclusion</h2>
      <!-- Experiment  -->
      <p> In this paper ...
      </p>
      <br>

      <div id="single-task">
        <!-- Performance on Single Tasks.. -->
         <br>
        <!-- <h3 class="title is-4">Success Examples of ACT on Single Tasks</h3>  -->
        <!-- <p> <b>Single Task Results</b>. ACT achieves an average success rate of 30.7% (Franka), 34.0% (Tien Kung), 55.3% (AgileX) and 38.0% (UR-5e).
        </p> -->
        <!-- <video width="640" height="360" controls>
          <source src="your-video.mp4" type="video/mp4">
      </video> -->
      </div>
      <br>



  </section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
              and <a href="https://eureka-research.github.io/">Eureka</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
